{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9358320c-f677-40d2-8c42-e7500b69ca14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fdb1d0ad-42eb-4f55-9e13-52463a688f64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "231aa900-d76e-4e00-bb8d-b7687880f19e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n",
      "Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n",
      "`config.hidden_activation` if you want to override this behaviour.\n",
      "See https://github.com/huggingface/transformers/pull/29402 for more details.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c971eff4b9d4dbbb4d1693df713958d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/python/3.10.13/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/python/3.10.13/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos>Write me a poem about Machine Learning.\n",
      "\n",
      "I’m not sure what you mean by “write” but I can tell that it is going to be an interesting one! Let us see how we go with this…\n",
      "\n",
      "\n",
      "<h2>What Is Machine Learning?</h2>\n",
      "\n",
      "Machine learning (ML) refers broadly as the study of algorithms and statistical models that computer systems use for machine learning, which means they learn without being explicitly programmed or designed in advance [1]. It has been around since 1950s when computers were first introduced into our lives through mainframes like IBM 704/IBM 7090 series machines at universities across America where these programs would run on them automatically after hours while students slept peacefully knowing their work was done before dawn came up again tomorrow morning – until someone figured out there might actually need some human interaction too sometimes so now most companies have started hiring people who know nothing whatsoever except programming languages such as Python because those are all pretty much required nowadays anyway even if your job doesn’t involve coding directly anymore; however don’t worry though cause once trained enough then anyone could easily pick things back up quickly thanks mostly due its simplicity compared other more complex ones available today including deep learning etcetera.. So yeah basically just think along lines similar concept wise rather than trying figure exactly whats involved here right off bat unless really interested want dive deeper later down road anyways let get onto next section shall start talking specifics details related topic itself instead focusing solely upon general overview overall idea behind whole thing from beginning end result perspective only way possible understand fully why something works well does good results achieve desired outcome accomplish goal set forth initially intended purpose thus making sense doing research project involving ML should definitely consider taking time explore subject matter thoroughly beforehand make informed decisions based facts figures data evidence gathered actual experience gained personal opinion preferences feelings thoughts opinions beliefs values attitudes behaviors habits routines customs traditions culture norms expectations standards rules regulations laws policies procedures guidelines requirements recommendations suggestions ideas concepts theories principles methods techniques tools resources materials supplies equipment facilities premises locations sites environments conditions circumstances situations contexts causes effects outcomes consequences impacts influences factors variables determinants reasons motivations incentives rewards punishments costs benefits risks hazards dangers liabilities obligations duties responsibilities authorities powers rights privileges immunities exemptions exclusions limitations restrictions controls checks balances safeguards protections guarantees assurances commitments pledges promises oaths vows affirmations declarations acknowledgements attestations verifications certifications authorizations permits licenses consents waivers releases discharges exonerations pardons commutations commuted sentences expungements amnesty paroles acquittals dismissals absolvings vacatur reversals remands stays postpones suspensions delays adjournments continuances recesses intermissions pauses interruptions gaps breaks lapses omissions voids absences non-appearances absentees leaves absence days absentness excused unexcused leave excuses explanations rationales justifications defenses counterarguments refutations disclaimers denials reservations objections protests appeals grievances complaints charges accusations allegations claims assertions disputes controversies proceedings litigation suits actions cases matters issues concerns problems challenges inquiries investigations enquiries queries interrogatories depositions affidavits testimonies statements admissions confessions retractions recantations retraction withdrawals withdrawal requests petitions applications submissions communications messages transmissions broadcasts signals sounds vibrations waves oscillations ripples tremors tsunamis earthquakes quakes landslides avalanches floods droughts storms hurricanes typhoons cyclones tornadoes hail sleet snow ice rain wind thunder lightning fog smog haze smoke dust ash soot vapor steam mist condensation dew frost rime glaze drizzle freezing precipitation melting snowfall slush thaw freeze melt evaporation sublimation deposition accretion sedimentation suspension crystallization solidification crystallisation crystal growth recrystallization phase transition metamorphism metamorphosis transformation transmutation fission fusion thermonuclear explosion nuclear reaction atomic bomb hydrogen bomb electromagnetic pulse EMP magnetic pulse MP high energy particle beam HEPB heavy ion accelerator HIA hadron collider HIAC hybrid laser plasma HLPP light gas laser LGL low energy electron positron LEP linear electron positron LEP lepton colliders LHC large hadron collider LHCC liquid helium cryostat LCLL liquefied carbon dioxide CCF cold cathode field emission CCE charge coupled device CCD capacitive coupling capacitor capacitance conductance resistance resistivity conductivity dielectric constant permittivity permeability susceptibility diamagnetism paramagnetism ferromagnetism ferrimagnetism antiferromagnetism superconductivity superfluidity Bose-Einstein condensate\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2b\")\n",
    "\n",
    "input_text = \"Write me a poem about Machine Learning.\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\")\n",
    "\n",
    "outputs = model.generate(\n",
    "    **input_ids, \n",
    "    max_length=800, \n",
    "    temperature=0.2, \n",
    "    # skip_special_tokens=True, \n",
    "    # clean_up_tokenization_spaces=True,\n",
    "    top_k=50,\n",
    "    top_p=0.95,\n",
    "    repetition_penalty=1.5\n",
    ")\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
